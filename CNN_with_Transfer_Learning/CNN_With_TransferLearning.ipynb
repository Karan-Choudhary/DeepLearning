{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## This is the basic generic template such that we can implement all the Transfer Learning algorithms by just importing the library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In transfer learning we use the state of art algorithms "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.models import Model,Sequential\r\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\r\n",
    "from tensorflow.keras.preprocessing import image\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
    "import numpy as np\r\n",
    "from glob import glob\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from tensorflow.keras.layers import Input , Dense, Lambda, Flatten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initially we have to use an image size of 224X224 because VGG16 was created in such a way that the input image size is actually 224X224"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "IMAGE_SIZE = [224,224]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train_path = 'Dataset/Train'\r\n",
    "test_path = 'Dataset/Test'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# add preprocessing layer to the front of VGG\r\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE +[3], weights='imagenet',include_top=False)\r\n",
    "# here we use 3 to because we have rgb images and this 3 represent the 3 Channels.\r\n",
    "# include_top = false represent that the last layer or the output layer is not add to this particular VGG model that we created."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# we don't have any need to train the existing layers of VGG or we can say, we don't have any need to train existig weights because those layers are already trained and weights are fixed.\r\n",
    "for  layer in vgg.layers:\r\n",
    "    layer.trainable = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# useful for getting nummber of classes\r\n",
    "folders = glob('Dataset/Train/*')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# our layers - we can add more if we want\r\n",
    "x = Flatten()(vgg.output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#  x = Dense(1000, activation = 'relu')(x)\r\n",
    "prediction = Dense(len(folders),activation='softmax')(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Create a model object\r\n",
    "model = Model(inputs = vgg.input, outputs  = prediction)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# view the structure of the model\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 125445    \n",
      "=================================================================\n",
      "Total params: 14,840,133\n",
      "Trainable params: 125,445\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# tell the model what cost and optimization method to use\r\n",
    "model.compile(\r\n",
    "    loss= 'categorical_crossentropy',\r\n",
    "    optimizer='adam',\r\n",
    "    metrics=['accuracy']\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\r\n",
    "shear_range=0.2,\r\n",
    "zoom_range=0.2,\r\n",
    "horizontal_flip=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "training_set = train_datagen.flow_from_directory('Dataset/Train',\r\n",
    "target_size=(224,224),\r\n",
    "batch_size=32,\r\n",
    "class_mode='categorical'\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 10 images belonging to 5 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# fit the model \r\n",
    "r = model.fit_generator(\r\n",
    "    training_set,\r\n",
    "    epochs=5,\r\n",
    "    steps_per_epoch=len(training_set)\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\kc510\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.7750 - accuracy: 0.3000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3455 - accuracy: 0.4000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1497 - accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3462 - accuracy: 0.9000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5484 - accuracy: 0.6000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now in this template we can use any Transfer Learning Model according to our need"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "578ce72e72bd9f13049fd4c13d9f5b1c81715c13ddea0a3c61ff70756cb5d6d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}